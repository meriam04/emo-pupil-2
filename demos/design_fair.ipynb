{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Fair Demo\n",
    "\n",
    "Welcome to the Emotion Watchers design fair! Here, we'll take a look into the end-to-end process of testing the emotion detection model, from data collection to inference. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/transparent_logo.png\" width=\"80\" height=\"80\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "If not already done, be sure to collect the facial expression videos and pupillometry data of the participant. \n",
    "\n",
    "In order for the data processing step to work with the given commands, ensure that:\n",
    "\n",
    "<input type='checkbox' > The video is saved in `C:\\Users\\meria\\Pictures\\Camera Roll`  <br>\n",
    "<input type='checkbox' > The video name follows the structure of `<partipant_id>_<emotion>.mp4` <br>\n",
    "<input type='checkbox' > The pupillometry data should be saved in `C:\\Users\\meria\\Documents\\GazePoint\\result` <br>\n",
    "<input type='checkbox' > The pupillometry file should be named `<participant_id>_all_gaze.csv` <br>\n",
    "<input type='checkbox' > The output_path (`\\data`) should be cleared <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"mfu\"\n",
    "emotion = \"calm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Once the data has been collected, it must be processed. \n",
    "\n",
    "The facial video is converted to a sequence of images and cropped to include only the face. The face can be detected either using the [Haar Cascade model](https://www.datacamp.com/tutorial/face-detection-python-opencv) or a UI. These images are then classified by the emotion and resized to be interpreted by the model.\n",
    "\n",
    "The pupillometry data is processed using MATLAB to remove outliers, such as blinking. Then the average of the left and right pupil dilations is outputted. Since the pupillometry data is discrete and has gaps after removing outliers, we created a continuous function from the data points using a [cubic spline](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html) interpolation technique.\n",
    "\n",
    "To process the data, run the command below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!python ..\\data_processing\\data_processing\\process_data.py \"C:\\Users\\meria\\Pictures\\Camera Roll\" ..\\data C:\\Users\\meria\\Documents\\GazePoint\\result -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the frames\n",
    "frame_dir = \"C:\\\\Users\\\\meria\\\\Pictures\\\\Camera Roll\"\n",
    "frame_path = Path(frame_dir) / f\"{id}_{emotion}\"\n",
    "\n",
    "# List all the frames in the directory\n",
    "frames = os.listdir(frame_path)\n",
    "mid = len(frames) // 2 \n",
    "frame = frames[mid]  # Get the middle frame\n",
    "\n",
    "# Display the middle frame\n",
    "display(Image(frame_path / frame))\n",
    "print(frame)\n",
    "\n",
    "# Define the directory containing the cropped frames\n",
    "cropped_dir = frame_path / \"cropped\"\n",
    "\n",
    "# List all the cropped frames in the directory\n",
    "cropped_frames = os.listdir(cropped_dir)\n",
    "cropped_frame = cropped_frames[len(cropped_frames) // 2]  # Get the middle cropped frame\n",
    "\n",
    "# Display the middle cropped frame\n",
    "display(Image(cropped_dir / cropped_frame))\n",
    "print(cropped_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pupil_plot_dir = Path(\"C:\\\\Users\\\\meria\\\\Documents\\\\GazePoint\\\\result\")\n",
    "plot_name = f\"pupil_{id}_{emotion}.pkl.png\"\n",
    "display(Image(pupil_plot_dir / plot_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "To integrate facial expressions and pupillometry into emotion prediction, two separate models were initially developed and later combined into an ensemble. The facial model was trained on cropped and grayscale facial images, while the pupillometry model was trained on sequences of pupil dilations. The fusion model is utilized solely for testing purposes.\n",
    "\n",
    "### Facial Model\n",
    "\n",
    "The facial model processes cropped facial images and offers predictions in two modes: binary and multiclass.\n",
    "\n",
    "- **Binary Mode**: Emotions are categorized as either positive or negative.\n",
    "- **Multiclass Mode**: Emotions are classified into several categories, including anger, fear, joy, fun, calm, happy, and sad.\n",
    "\n",
    "The current implementation utilizes the multiclass mode.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The facial model employs the following architecture:\n",
    "\n",
    "- **Convolutional Layers**: These layers are responsible for extracting features from the input images.\n",
    "- **Linear Layers**: Following the feature extraction, linear layers make predictions based on the extracted features.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/face_model.png\" height=200>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <i> Figure: Architecture of the Facial Model </i>\n",
    "</p>\n",
    "\n",
    "### Pupillometry Model\n",
    "\n",
    "The pupillometry model focuses on binary classification at present. However, it can be extended to multiclass classification by incorporating binary predictions into corresponding emotions.\n",
    "\n",
    "#### Classification Mode\n",
    "\n",
    "- **Binary Classification**: The model currently operates in binary classification mode.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The model prioritizes analyzing changes in pupil dilation rather than focusing solely on absolute dilation values. The architecture comprises:\n",
    "\n",
    "- **LSTM Layers**: These layers retain memory over a sequence of dilations, capturing temporal dynamics.\n",
    "- **Convolutional Layers**: Responsible for extracting features from pupil dilation sequences.\n",
    "- **Linear Layers**: These layers make predictions based on the extracted features.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/pupil_model.png\" height=200>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <i> Figure: Architecture of the Pupillometry Model </i>\n",
    "</p>\n",
    "\n",
    "### Fusion Model\n",
    "\n",
    "The fusion model combines the predictions generated by the facial and pupillometry models and selects the emotion with the highest total confidence. This integration allows for a comprehensive assessment by leveraging insights from both modalities.\n",
    "\n",
    "The fusion model's classification mode, whether binary or multiclass, is determined by the combined outputs of the facial and pupillometry models. If both models operate in binary mode, the fusion model will also produce binary predictions. Conversely, if either or both models utilize multiclass classification, the fusion model will follow suit.\n",
    "\n",
    "By synthesizing information from both facial expressions and pupillometry, the fusion model provides a more robust and nuanced approach to emotion prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!python ..\\models\\models\\fusion.py C:\\Users\\meria\\Documents\\GazePoint\\result ..\\data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_dir = \"..\\data\"\n",
    "frame_path = Path(frame_dir) / f\"{emotion}\"\n",
    "\n",
    "frames = os.listdir(frame_path)\n",
    "mid = len(frames) // 2\n",
    "frame = frames[mid]\n",
    "print(frame)\n",
    "Image(frame_path / frame)\n",
    "\n",
    "#TODO: add code to output what emotion was predicted for this image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "In order to gain insight on the model's predictions, we generate a **confusion matrix**, seen below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"..\\models\\models\\confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cleanup\n",
    "# move to archive and delete from there (bc of the weird thing)\n",
    "# in result, also move to archive\n",
    "# delete everything in data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
